{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive Machine Learning Pipelines with scikit-learn - Part 1\n",
    "\n",
    "The objective is to build a scikit-learn pipeline that takes in raw data, transforms many different subsets of data, learns from it, tunes hyperparameters and is then able to make predictions. The final outcome will be a single sckit-learn estimator that does it all. This estimator will then be able to be used in the future to make predictions in a single step.\n",
    "\n",
    "This tutorial includes information on how to:\n",
    "* Categorize columns into distinct groups\n",
    "* Apply independent transformations to each column grouping\n",
    "* Create a single pipline that handles all the steps of transforming, modeling, validating, and predicting\n",
    "* Build custom transformers to create new features\n",
    "* Save our final model to disk to be used to make future predictions\n",
    "\n",
    "## Focus on transformations\n",
    "Although this builds a complete machine learning pipeline, most of it will focus on how to transform and prepare the data for the machine learning models.\n",
    "\n",
    "## Assume fundamentals of scikit-learn\n",
    "This tutorial assumes you are familiar with doing machine learning with scikit-learn. At a minimum, you need to know what a scikit-learn estimator is and how it behaves\n",
    "\n",
    "## Prior Issues with transformations in scikit-learn\n",
    "Up until the release of scikit-learn version 0.20 in September, 2018, there was no easy way to apply separate transformations to different subsets of the data. Additionally burdensome, was encoding of categorical features. For instance, building a single pipeline to handle input data that contained a mix of continuous and categorical variables was not trivial and a huge issue. Many divergent workflows were built to accommodate such transformations.\n",
    "\n",
    "The addition of the `ColumnTransformer` and upgrade to the `OneHotEncoder` in version 0.20 alleviated these painful issues. Building an entire pipeline in scikit-learn was not pleasant before these additions. A previous post of mine details the exciting new workflow that became possible.\n",
    "\n",
    "## Real-World Data\n",
    "In this post, we will work with the [Ames, Iowa housing dataset][0] from a popular Kaggle competition. You are given a dataset with 79 features with the objective of learning a model to predict the sale price.\n",
    "\n",
    "[0]: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "## Begin by reading in data\n",
    "Let's begin by reading our training data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "housing = pd.read_csv('data/train.csv')\n",
    "housing.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the sale price\n",
    "The target variable to predict is the sale price. We will remove it and assign to the variable `y` with the `pop` method, which modifies the DataFrame in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell twice will cause an error since the columns are no longer in the DataFrame\n",
    "y = housing.pop('SalePrice').values\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the data dictionary to gain a deeper understanding of the problem\n",
    "\n",
    "A very useful data dictionary is provided that gives descriptions on each column in the dataset. We will rely on it to understand the meaning of the columns and help with making more logical transformations.\n",
    "\n",
    "Its contents may be printed out directly into the notebook for easy access. For now, look at the first column. It's composed of numeric values, but they are just codes for a type of house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(open('data/data_description.txt').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying columns into groups\n",
    "One of the most important tasks during this tutorial is going to be the classification of each column into a particular group. Each group will undergo its own set of transformations before being used as input in the machine learning model. A commonly taught approach is to classify each column as either **categorical** or **continuous**. Values in categorical columns are discrete and typically the total number of categories is known. Continuous columns are always numeric and not limited to a known set of values.\n",
    "\n",
    "### Different transformations for each group\n",
    "Categorical columns are processed differently than continous ones with each group needing its own set of transformations. For instance, we may want to fill in missing values with the most frequent for categorical, but the mean for continuous. Categorical columns are often one-hot encoded, while continuous columns are often standardized. As we will see, there are different groups of columns than just categorical or continuous and each column group will have its own set of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Mini Machine Learning Pipeline\n",
    "Before we embark on our massive machine learning pipeline, we'll create a much simpler one that consists of a few columns that are either categorical or continuous. Creating this small pipeline will help understand the larger one.\n",
    "\n",
    "### Select columns for each group\n",
    "We'll begin by assigning a few categorical and continuous columns as lists. The columns here are chosen arbirtrarily as the focus is going to be on applying the transformations and building the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Neighborhood', 'LotShape', 'OverallQual', 'MasVnrType']\n",
    "cont_cols = ['GrLivArea', 'GarageArea', 'LotFrontage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline for each column group\n",
    "Each group will go through two transformations. For the categorical columns, the missing values will be filled with the most frequent and then one-hot encoded. For the continuous columns, the missing values will be filled with the mean and then standardized. A scikit-learn `Pipeline` can be used whenever there are two or more transformations that are needed to be applied in succession.\n",
    "\n",
    "Before we biuld the pipeline, we will import each of the transformers and instantiate them. Notice that the `OneHotEncoder` is constructed with the `handle_unknown` parameter set to 'ignore'. This will help us when predicting new data that have categories not seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "si_cat = SimpleImputer(strategy='most_frequent')\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "si_cont = SimpleImputer(strategy='mean')\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the scikit-learn `Pipeline` to create the actual pipelines for each column group. To instantiate a `Pipeline`, create a list of two-item tuples where each tuple consists of a **name** and the **transformer**. The name is an arbitrary string that may be used to reference the transformer at a later time. Below, we create two pipelines, `cat_pipe` and `cont_pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cat_steps = [\n",
    "    ('si', si_cat),\n",
    "    ('ohe', ohe)\n",
    "]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "cont_steps = [\n",
    "    ('si', si_cont),\n",
    "    ('ss', ss)\n",
    "]\n",
    "cont_pipe = Pipeline(cont_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, scikit-learn will apply the transformations in a pipeline to all of the columns. But, we are only interested in passing the categorical columns through the categorical pipeline and the continuous columns through the continuous pipeline. The excellent `ColumnTransformer` allows us to do this. We instantiate it with a list of three-item tuples where each tuple consists of a **name** of the transformation, the **transformer**, and a list of **columns** to apply the transformation to. In our case, the transformer is a pipeline of individual transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transformers = [\n",
    "    ('cat_cols', cat_pipe, cat_cols),\n",
    "    ('cont_cols', cont_pipe, cont_cols)\n",
    "]\n",
    "ct = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the ColumnTransformer\n",
    "At this point, we have constructed the machinery to do the transformations. We haven't actually done any transformations, but we are ready to do so. The following shows how our data would flow at this point. Our raw data would be passed to the `ColumnTransformer` which will send the categorical column to the categorical pipeline and the continuous columns to the continuous pipelines. Each pipeline will apply two successive transformations to the data. After each pipeline has completed, the `ColumnTransformer` concatenates the data back together to form a singe transformed dataset.\n",
    "\n",
    "![](images/simple_columntransformer.png)\n",
    "\n",
    "### Passing the data through the `ColumnTransformer`\n",
    "\n",
    "Let's pass our data through the `ColumnTransformer` to obtain our final transformed dataset. Only the columns that appear in either the `cat_cols` or `cont_cols` lists will be transformed. Any other columns will be dropped. Pass in the pandas DataFrame to the `fit_transform` method and you will be returned a numpy array of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = ct.fit_transform(housing)\n",
    "type(X_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the shape of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transformed 7 columns and were returned 46. This is entirely due to one-hot encoding. All the other transformers mapped each input column to exactly one output column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one more pipeline to do machine learning\n",
    "Our data is now ready to be fed into a machine learning model. We could use the variable `X_t` that was created above, but instead, we will build another pipeline where the first step passes the data through the `ColumnTransformer` and the second passes the transformed data to the machine learning model. We'll use Ridge Regression for learning. Let's create this final two-step pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "steps = [\n",
    "    ('ct', ct),\n",
    "    ('ridge', ridge)\n",
    "]\n",
    "\n",
    "final_pipe = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the final pipeline\n",
    "\n",
    "Our final pipeline is a bit complex. It consists of only two steps, but the ColumnTransformer in the first step contains two separate pipelines and each of those pipelines contains two transformations. The image below shows how the data is processed.\n",
    "\n",
    "![](images/final_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Let's use this pipeline to train a Ridge regression model. We simply pass the original DataFrame to the `fit` method which will run all the transformations and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe.fit(housing, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "After training, we can now make predictions on new dataets that have the same column names as the original. Let's read in the test dataset and assign the Id column to its own variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test = pd.read_csv('data/test.csv')\n",
    "housing_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass in the test set to the pipeline's `predict` method to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_pipe.predict(housing_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model to disk for future use\n",
    "We can preserve our trained pipeline exactly as it is by saving it to disk with help from the `joblib` library `dump` function. See [joblib documentation][0] for more information. Pass it the pipeline and a name for the new file.\n",
    "\n",
    "[0]: https://joblib.readthedocs.io/en/latest/persistence.html#persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(final_pipe, 'models/minipipeline_ridge.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the saved model\n",
    "We can retrieve the model from disk with the `load` function. It has preserved every step of the pipeline. We test that the results are the same by testing that the predictions are the same as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe_new = joblib.load('models/minipipeline_ridge.joblib')\n",
    "y_pred_new = final_pipe_new.predict(housing_test)\n",
    "(y_pred == y_pred_new).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a prediction for each test observation and can submit them to Kaggle to get scored and ranked against the other competitors.\n",
    "\n",
    "### Create a csv of Id and SalePrice\n",
    "We need to submit a csv file of the row Id and our predicted sale price. We do so with the DataFrame constructor to create a two-column DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub01 = pd.DataFrame({'Id': housing_test['Id'], 'SalePrice': y_pred})\n",
    "sub01.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File naming and directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can export our DataFrame as a csv. I strongly recommend creating a submissions folder within the data folder and within that folder a new folder for each date that you track submissions. Within that folder is where the submission files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub01.to_csv('data/submissions/20190710/sub01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our directory structure for the data takes the following shape:\n",
    "\n",
    "![](images/dir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a submission to Kaggle from python\n",
    "\n",
    "Kaggle has kindly provided a [python library][0] (`pip install kaggle`) to make submissions programmatically. Visit [this section of the documentation][1] to learn how to get your credentials.\n",
    "\n",
    "[0]: https://github.com/Kaggle/kaggle-api\n",
    "[1]: https://github.com/Kaggle/kaggle-api#api-credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit our csv by passing the submission function our file location, a message, and the competition name. It's important to give a good descriptive message so that you can remember how that particular submission was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/submissions/20190710/sub01.csv'\n",
    "message = '''\n",
    "One hot encoded four categorical columns and standardized three continuous\n",
    "columns. Modeled with ridge regression with alpha=1\n",
    "'''\n",
    "competition = 'house-prices-advanced-regression-techniques'\n",
    "# kaggle.api.competition_submit(file, message, competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve score\n",
    "We can retrieve a list of all of our prior scores with the following function call. The most recent submission will be the first item in the list which we print to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = kaggle.api.competitions_submissions_list(competition)\n",
    "all_submissions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition uses root mean squared error of the logged housing prices. Our score is `0.19`. Unfortunately there is no easy way to get our place on the leaderboard, but you can just navigate [directly to the Kaggle leaderboard][0] instead.\n",
    "\n",
    "[0]: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline vs ColumnTransformer\n",
    "It may not be crystal clear what the difference is between the `Pipeline` and `ColumnTransformer`. The simplest distinction is to think of the `Pipeline` as moving all of the data **vertically** in succession from one transformer to the next. The `ColumnTransformer` splits data **horizontally** into multiple subsets. Each of these subsets gets applied a transformation that is independent of all the other subsets of data. All the transformed subsets are then concatenated together to form a single dataset.\n",
    "\n",
    "In a `Pipeline`, all columns will be passed into each estimator with the result being passed to the next estimator. A `Pipeline` allows for the last step to be a machine learning model whereas the `ColumnTransformer` only allows transformers.\n",
    "\n",
    "### All steps in one cell\n",
    "All the above steps for the our mini machine learning pipeline were written in different cells, which probably makes it harder to see the full structure of the program as it would outside of a Jupyter Notebook/Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_cat = SimpleImputer(strategy='most_frequent')\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "si_cont = SimpleImputer(strategy='mean')\n",
    "ss = StandardScaler()\n",
    "\n",
    "cat_cols = ['Neighborhood', 'LotShape', 'OverallQual', 'MasVnrType']\n",
    "cont_cols = ['GrLivArea', 'GarageArea', 'LotFrontage']\n",
    "\n",
    "cat_steps = [('si', si_cat), ('ohe', ohe)]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "cont_steps = [('si', si_cont), ('ss', ss)]\n",
    "cont_pipe = Pipeline(cont_steps)\n",
    "\n",
    "transformers = [('cat_cols', cat_pipe, cat_cols), ('cont_cols', cont_pipe, cont_cols)]\n",
    "ct = ColumnTransformer(transformers)\n",
    "\n",
    "steps = [('ct', ct), ('ridge', ridge)]\n",
    "final_pipe = Pipeline(steps)\n",
    "final_pipe.fit(housing, y)\n",
    "y_pred = final_pipe.predict(housing_test)\n",
    "sub01 = pd.DataFrame({'Id': housing_test['Id'], 'SalePrice': y_pred})\n",
    "sub01.to_csv('data/submissions/20190710/sub01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive Machine Learning Pipelines - Part 2\n",
    "We will move on to building the massive machine learning pipeline. The overall architecture will look similar to the mini-pipeline from above with the major difference being the number of distinct column groups. Each of the column groupings we create will have its own pipeline.\n",
    "\n",
    "## Create column groupings\n",
    "For this massive pipeline, we will use all of the columns. Typically, it's a good idea to do exploratory data analysis first to manually inspect the columns and possibly select a subset to model on. We will forgo this step and go straight into pipeline construction. Further along in the tutorial we will do some data inspection and feature engineering.\n",
    "\n",
    "Below, we separate the data into five separate column groupings. The data type of each column is either numeric or string. Both numeric and string columns can be categorical, but only numeric data can be continuous. Categorical data is further subdivided into nominal (no natural ordering) or ordinal (has a natural ordering - basement quality for example). The data dictionary was used to help classify each column correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_nomial = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n",
    "              'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "              'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
    "              'Heating', 'CentralAir', 'Electrical', 'GarageType', 'GarageFinish', 'PavedDrive',\n",
    "              'MiscFeature', 'SaleType', 'SaleCondition']\n",
    "str_ordinal = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "               'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'GarageQual', 'GarageCond',\n",
    "               'PoolQC', 'Fence', 'FireplaceQu']\n",
    "\n",
    "numeric_nominal = ['MSSubClass', 'MoSold', 'YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\n",
    "numeric_ordinal = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "                   'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', ]\n",
    "numeric_cont = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n",
    "                'TotalBsmtSF','1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', \n",
    "                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "                'MiscVal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal vs Ordinal\n",
    "Most of the time it is easy to determine whether a categorical column is nominal or ordinal. The neighborhood column, for instance has no natural ordering and would therefore be classified as nominal. \n",
    "\n",
    "But with a column like LotShape, it isn't so clear. The values are Reg (Regular), IR1 (Slightly Irregular), IR2 (Moderately Irregular), and IR3 (Irregular). The values appear to have an order with Reg being the 'best' and IR3 the 'worst'. But, without being an expert in this field, it's probably safer to assume less and treat it as a nominal.\n",
    "\n",
    "### One-Hot encoding all categorical columns\n",
    "While scikit-learn does provide an `OrdinalEncoder` to encode ordinal features, we will choose opt to one-hot encode them instead. This is done for a couple reasons. First, ordinal encoding places a stricter assumption on the data, that each category is worth precisely one more than the prior one. Second, we would need to provide an ordered list of categories for each column and that would be quite tedious. So, partly out of laziness, we will one-hot encode all categorical columns regardless if they are nominal or ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline steps for each column group\n",
    "\n",
    "We will now create a pipeline for each subset of columns. We will fill missing values with the most frequent for all of the categorical columns and with the median for the continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_nominal_steps = [\n",
    "    ('si', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "]\n",
    "str_orinal_steps = [\n",
    "    ('si', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "]\n",
    "numeric_nominal_steps = [\n",
    "    ('si', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "]\n",
    "numeric_ordinal_steps = [\n",
    "    ('si', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "]\n",
    "numeric_cont_steps = [\n",
    "    ('si', SimpleImputer(strategy='median')),\n",
    "    ('ss', StandardScaler())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline for each column group\n",
    "We instantiate each pipeline with the steps (list of two-item tuples) from above. Notice, that all pipelines are identical except for the continuos one. We could have used just two pipelines to transform the data like we did in our first attempt. In upcoming examples, we will see different sets of transformations applied to each pipeline. Regardless, it is still useful to see how different columns many be categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_nominal_pipe = Pipeline(str_nominal_steps)\n",
    "str_ordinal_pipe = Pipeline(str_orinal_steps)\n",
    "numeric_nominal_pipe = Pipeline(numeric_nominal_steps)\n",
    "numeric_ordinal_pipe = Pipeline(numeric_ordinal_steps)\n",
    "numeric_cont_pipe = Pipeline(numeric_cont_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ColumnTransformer\n",
    "We instantiate our `ColumnTransformer` with a list of three-item tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [\n",
    "    ('str_nominal_pipe', str_nominal_pipe, str_nomial),\n",
    "    ('str_ordinal_pipe', str_ordinal_pipe, str_ordinal),\n",
    "    ('numeric_nominal_pipe', numeric_nominal_pipe, numeric_nominal),\n",
    "    ('numeric_ordinal_pipe', numeric_ordinal_pipe, numeric_ordinal),\n",
    "    ('numeric_cont_pipe', numeric_cont_pipe, numeric_cont)\n",
    "]\n",
    "ct = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one last pipeline to add machine learning\n",
    "A final pipeline is created to connect the transformed data to the Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_steps = [\n",
    "    ('ct', ct),\n",
    "    ('ridge', Ridge())\n",
    "]\n",
    "final_pipe2 = Pipeline(final_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and predict\n",
    "We can now pass our data into the final pipeline to transform and ultimately train our ridge regression model. We follow this up by predicting the sale price for the test set and saving a new submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe2.fit(housing, y)\n",
    "y_pred = final_pipe2.predict(housing_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate submission to kaggle\n",
    "We can define a function to create the csv, submit the prediction, and return the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_kaggle(model, X_test, file, message):\n",
    "    y_pred = model.predict(X_test)\n",
    "    sub = pd.DataFrame({'Id': X_test['Id'], 'SalePrice': y_pred})\n",
    "    sub.to_csv(file, index=False)\n",
    "    kaggle.api.competition_submit(file, message, competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/submissions/20190710/sub02.csv'\n",
    "message = '''\n",
    "One hot encoded all categorical columns and standardized all continuous\n",
    "columns. Modeled with ridge regression with alpha=1\n",
    "'''\n",
    "# submit_kaggle(final_pipe2, housing, file, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive Machine Learning Pipelines - Part 3\n",
    "### Cross Validation\n",
    "\n",
    "It's pretty disappointing to see all that hard work separating columns into groups doing absolutely nothing for our score. Since Kaggle only allows 10 submissions per day, it would have been nice to know that our model wouldn't show an improvement before sending our submission.\n",
    "\n",
    "We can get a fairly good idea of how our model is going to perform by doing cross validation. Below, we perform K-Fold cross validation with 5 folds. By default, the r-squared value will be returned as the score. Instead, we can return the negative mean of the squared log error, which is nearly the same error calculation that Kaggle uses.\n",
    "\n",
    "In most all other machine learning contexts, the root mean squared error is reported as a positive number. scikit-learn reports it as a negative number. This is strange, but is done because it considers a greater score to be better and ranks models based on their score when grid searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning - this might not work for other random states\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "scores = cross_val_score(final_pipe2, housing, y, \n",
    "                         cv=kf, scoring='neg_mean_squared_log_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To report the same metric as Kaggle, we will negate the scores, take the square root, and then average all of the scores. Because we are shuffling the data during cross validation, you'll get a different score each time, but it should be around .18, which indicates that there would be no improvement over the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the target variable\n",
    "Instead of using the actual sale price as the target variable, we can instead use the log of it. This is common practice when modeling data that with a large range of values. Kaggle is already scoring based on the log of the sale price so it makes sense to use it directly. \n",
    "\n",
    "scikit-learn provides the `TransformedTargetRegressor` meta-estimator to automate the transforming of any continuous target variable. We pass it our pipeline, the transformation function applied to the target and a function to invert the transformation. The model will be trained against the transformed target variable (the log of the sale price in our example). When predicting, the inverse function will be used to transform the target back to the original units.\n",
    "\n",
    "Let's use this new estimator to train our model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "final_pipe2_ttr = TransformedTargetRegressor(final_pipe2, func=np.log, inverse_func=np.exp)\n",
    "final_pipe2_ttr.fit(housing, y)\n",
    "final_pipe2_ttr.predict(housing_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we verified the model can make predictions, let's use cross validation to get a estimate a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(final_pipe2_ttr, housing, y, \n",
    "                         cv=kf, scoring='neg_mean_squared_log_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score should have dropped to around .16 which gives us confidence that we will have some improvement over the original. Let's make our submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/submissions/20190710/sub03.csv'\n",
    "message = 'Same as 2, but trainined on log of y.'\n",
    "# submit_kaggle(final_pipe2_ttr, housing_test, file, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our score from kaggle is .14 which is better than what was predicted by cross validation and far better than our original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = kaggle.api.competitions_submissions_list(competition)\n",
    "all_submissions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "Our model with all of the columns produced a similar score as the first trained on just a fraction of the columns when modeled against the actual sales price. This is likely due to the extreme flexibility that we have given the model. We used all 79 variables and one-hot encoded most of them, which created an enormous number of features. Let's see exactly how many features were created by running our data just through the `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.transform(housing).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built our model on input data with over 600 features or nearly half the total number of observations. If we are to continue to use this high number of features, we'll need to choose a different type of model or apply constraints to this one.\n",
    "\n",
    "### Regularization\n",
    "Ridge regression constrains the parameters of the model by applying a penalty term (`alpha`) preventing the model from learning too much from the training data. We use the `GridSearchCV` meta-estimator to find the best possible `alpha` from 100 given between .01 and 1000. By default, `alpha` is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'regressor__ridge__alpha': np.logspace(-2, 3, 100)}\n",
    "gs = GridSearchCV(final_pipe2_ttr, param_grid, cv=kf, scoring='neg_mean_squared_log_error')\n",
    "gs.fit(housing, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the best value for alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the score associated with that alpha which is still right around .15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign the best model trained to its own variable and then get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe2_ttr_best = gs.best_estimator_\n",
    "y_pred = final_pipe2_ttr_best.predict(housing_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the best value for alpha, our ridge regression produced just about the same score. We won't bother submitting it to Kaggle. Let's try reducing the total number of features instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All of the above represents a fairly 'vanilla' massive machine learning pipeline. Summarizing, we have done the following:\n",
    "\n",
    "* Classified each column as either string or numeric and as either nominal, ordinal, or continuous\n",
    "* Built a pipeline for each of these 5 column groupings by filling in missing values and either one-hot encoding or standardizing\n",
    "* Used cross validation to estimate the root mean squared log error on the test set\n",
    "* Modeled on the log of the sale price\n",
    "* Used grid search to find the optimal value for the penalty term for ridge regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
