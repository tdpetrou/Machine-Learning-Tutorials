{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's new integration with Pandas\n",
    "Scikit-Learn will make one of its biggest upgrades in recent years with its mammoth version 0.20 release. For many data scientists, a typical workflow consists of using Pandas to do exploratory data analysis before moving to scikit-learn for machine learning. This new release will make the process simpler, more feature-rich, robust, and standardized.\n",
    "\n",
    "## Summary and goals of this article\n",
    "This article is aimed at those that use Scikit-Learn as their machine learning library but depend on Pandas as their data exploratory and preparation tool. \n",
    "\n",
    "* It assumes you have some familiarity with both Scikit-Learn and Pandas\n",
    "* We explore the new ColumnTransformer estimator, which allows us to apply separate transformations to different subsets of your data in parallel before concatenating the results together.\n",
    "* A major pain point for users (and in my opinion the worst part of Scikit-Learn) was preparing a pandas DataFrame with string values in its columns. This process should become much more standardized.\n",
    "* The OneHotEncoder estimator was given a nice upgrade to encode columns with string values. \n",
    "* To help with one hot encoding, we use the new SimpleImputer estimator to fill in missing values with constants\n",
    "* We will build a custom estimator that does all the \"basic\" transformations on a DataFrame instead of relying on the built-in Scikit-Learn tools. This will also transform the data with a couple different features not present within Scikit-Learn.\n",
    "* Finally, we explore binning numeric columns with the new KBinsDiscretizer estimator.\n",
    "\n",
    "### A note before we get started\n",
    "This tutorial is provided as a preview of things to come. The final version 0.20 has not been released. It is very likely that this tutorial will be updated at a future date to reflect any changes.\n",
    "\n",
    "### Continuing…\n",
    "For those that use Pandas as as their exploratory and preparation tool before moving to Scikit-Learn for machine learning, you are likely familiar with the non-standard process of handling columns containing string columns. Scikit-Learn's machine learning models require the input to be a two dimensional data structure of numeric values. No string values are allowed. Scikit-Learn never provided a canonical way to handle columns of strings, a very common occurrence in data science.\n",
    "\n",
    "This lead to numerous tutorials all handling string columns in their own way. Some solutions included turning to Pandas get_dummies function. Some used Scikit-Learn'sLabelBinarizer which does one-hot encoding, but was designed for labels (the target variable) and not for the input. Others created their own custom estimators. Even entire packages such as [sklearn-pandas][1] were built to support this trouble spot. This lack of standardization made for a painful experience for those wanting to build machine learning models with string columns.\n",
    "\n",
    "Furthermore, there was poor support for making transformations to specific columns and not to the entire dataset. For instance, it's very common to standardize continuous features but not categorical features.  This will now become much easier.\n",
    "\n",
    "### Upgrading to version 0.20\n",
    "The first release candidate for version 0.20 was put out just a couple days ago. You can install it with either conda:\n",
    "\n",
    "> `conda install scikit-learn=0.20rc1 -c conda-forge/label/rc -c conda-forge`\n",
    "\n",
    "or pip:\n",
    "\n",
    "> `pip install - pre scikit-learn`\n",
    "\n",
    "# Introducing `ColumnTransformer` and the upgraded `OneHotEncoder`\n",
    "With the upgrade to version 0.20, many workflows from Pandas to Scikit-Learn should start looking similar. The `ColumnTransformer` estimator applies a transformation to a specific subset of columns of your Pandas DataFrame (or array).\n",
    "\n",
    "The `OneHotEncoder` estimator is not new but has been upgraded to encode string columns. Before, it only encoded columns containing numeric categorical data.\n",
    "\n",
    "Let's see how these new additions work to handle string columns in a Pandas DataFrame.\n",
    "\n",
    "## Kaggle Housing Dataset\n",
    "One of Kaggle's beginning machine learning competitions is the [Housing Prices: Advanced Regression Techniques][2]. The goal is to predict housing prices given about 80 features. There are a mix of continuous and categorical columns. You can download the data from the website or use their [command line tool][3] (which is very nice).\n",
    "\n",
    "## Inspect the data\n",
    "Let's read in our DataFrame and output the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('data/housing/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the target variable from the training set\n",
    "The target variable is `SalePrice` which we remove and assign as an array to its own variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.pop('SalePrice').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding a single string column\n",
    "To start off, let's encode a single string column, `HouseStyle`, which has values for the exterior of the house. Let's output the unique counts of each string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = train['HouseStyle'].value_counts()\n",
    "vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 8 unique values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Gotcha - Must have 2D data\n",
    "Most Scikit-Learn estimator require that data be strictly 2-dimensional. If we select the column above as `train['HouseStyle']`, this technically creates a Pandas Series which is a single dimension of data. We can force Pandas to create a one-column DataFrame, by passing a single-item list to the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train = train[['HouseStyle']].copy()\n",
    "hs_train.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, Instantiate, Fit - The three-step process for each estimator\n",
    "The scikit-learn API is consistent for all estimators and uses a three-step process to train or fit the data. \n",
    "\n",
    "1. Import the estimator we want from the module its located in\n",
    "1. Instantiate the estimator possibly changing its defaults\n",
    "1. Fit the estimator to the data. Possibly transform the data to its new space if need be.\n",
    "\n",
    "Below, we import `OneHotEncoder`, instantiate it and ensure that we get a dense (and not sparse) array returned and then encode our single column with the `fit_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "hs_train_transformed = ohe.fit_transform(hs_train)\n",
    "hs_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it has encoded each unique value as its own binary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have a NumPy array. Where are the column names?\n",
    "Notice that our output is a NumPy array and not a Pandas DataFrame. Scikit-Learn was not originally built to be directly integrated with Pandas. All Pandas objects are converted to NumPy arrays internally and NumPy arrays are always returned after a transformation.\n",
    "\n",
    "We can still get our column name from the `OneHotEncoder` object through its `get_feature_names` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ohe.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying our first row of data is correct\n",
    "It's good to verify that our estimator is properly working. Let's look at the first row of encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row0 = hs_train_transformed[0]\n",
    "row0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encodes the 6th value in the array as 1. Let's use boolean indexing to reveal the feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[row0 == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify that the first value in our original DataFrame column is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `inverse_transform` to automate this\n",
    "Just like most transformer objects, there is an `inverse_transform` method that will get you back your original data. Here we must wrap `row0` in a list to make it a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.inverse_transform([row0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify all values by inverting the entire transformed array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_inv = ohe.inverse_transform(hs_train_transformed)\n",
    "hs_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(hs_inv, hs_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying transformation to the test set\n",
    "Whatever transformation we do to our training set, we must apply to our test set. Let's read in the test set and get the same column and apply our transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/housing/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test = test[['HouseStyle']].copy()\n",
    "hs_test_transformed = ohe.transform(hs_test)\n",
    "hs_test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should again get 8 columns and we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example works nicely, but there are multiple cases where we will run into problems. Let's examine them now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trouble area #1 - Categories unique to the test set\n",
    "What happens if we have a home with a house style that is unique to just the test set? Say something like `3Story`. Let's change the first value of the house styles and see what the default is from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test = test[['HouseStyle']].copy()\n",
    "hs_test.iloc[0, 0] = '3Story'\n",
    "print(hs_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(hs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error: Unknown Category\n",
    "By default, our encoder will produce an error. This is likely what we want as we need to know if there are unique strings in the test set. If you do have this problem then there could be something much deeper that needs investigating. For now, we will ignore this problem and encode this row as all 0's by setting the `handle_unknown` parameter to 'ignore' upon instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "ohe.fit(hs_train)\n",
    "\n",
    "hs_test_transformed = ohe.transform(hs_test)\n",
    "hs_test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the first row is all 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test_transformed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trouble area #2 - Missing Values in test set\n",
    "If you have missing values in your test set (NaN or None), then these will be ignored as long as `handle_unknown` is set to 'ignore'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test = test[['HouseStyle']].copy()\n",
    "hs_test.iloc[0, 0] = np.nan\n",
    "hs_test.iloc[1, 0] = None\n",
    "print(hs_test.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test_transformed = ohe.transform(hs_test)\n",
    "hs_test_transformed[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trouble area #3 - Missing Values in training set\n",
    "Missing values in the training set is more of an issue. As of now, the `OneHotEncoder` estimator cannot fit with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train = hs_train.copy()\n",
    "hs_train.iloc[0, 0] = np.nan\n",
    "hs_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "ohe.fit_transform(hs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if there was an option to ignore them like what happens when transforming the test set above. But this doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Must impute missing values\n",
    "For now, we must impute the missing values. The old `Imputer` from the preprocessing module got deprecated. A new module, `impute`, was formed in its place, with a new estimator `SimpleImputer` and a new strategy, 'constant'. By default, using this strategy will fill missing values with the string 'missing_value'. We can choose what to set it with the fill_value parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train = train[['HouseStyle']].copy()\n",
    "hs_train.iloc[0, 0] = np.nan\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "hs_train_imputed = si.fit_transform(hs_train)\n",
    "hs_train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can then encode as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train_transformed = ohe.fit_transform(hs_train_imputed)\n",
    "hs_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that we now have an extra column and and an extra feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply both transformations to test set\n",
    "We can manually apply each of the two steps above in order like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test = test[['HouseStyle']].copy()\n",
    "hs_test.iloc[0, 0] = 'unique value to test set'\n",
    "hs_test.iloc[1, 0] = np.nan\n",
    "\n",
    "hs_test_imputed = si.transform(hs_test)\n",
    "hs_test_transformed = ohe.transform(hs_test_imputed)\n",
    "hs_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why just the `transform` method for the test set?\n",
    "When transforming the test set, its important to just call the `transform` method and not `fit_transform`. When we ran `fit_transform` on the training set, Scikit-Learn found all the necessary information it needed in order to transform any other dataset containing the same column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a `Pipeline` instead\n",
    "Scikit-Learn provides a Pipeline transformer and estimator that takes a list of transformations and applies them in succession. You can also run a machine learning model as the final estimator. Here we simply impute and encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step is a two-item tuple consisting of a string that labels the step and the instantiated estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_step = ('si', SimpleImputer(strategy='constant', fill_value='MISSING'))\n",
    "ohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "steps = [si_step, ohe_step]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "hs_train = train[['HouseStyle']].copy()\n",
    "hs_train.iloc[0, 0] = np.nan\n",
    "\n",
    "hs_transformed = pipe.fit_transform(hs_train)\n",
    "hs_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is easily transformed by passing through each step of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_test = test[['HouseStyle']].copy()\n",
    "hs_test_transformed = pipe.transform(hs_test)\n",
    "hs_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple String Columns\n",
    "Encoding multiple string columns is not a problem. Select the columns you want and then pass the new DataFrame through the pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = ['RoofMatl', 'HouseStyle']\n",
    "string_train = train[string_cols]\n",
    "print(string_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_train_transformed = pipe.fit_transform(string_train)\n",
    "string_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get individual pieces of the pipeline\n",
    "It is possible to get each individual transformer through its name. In this instance, we get the one-hot encoder so that we can output the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = pipe.named_steps['ohe']\n",
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the new ColumnTransformer to choose columns\n",
    "The brand new `ColumnTransformer` (part of the new compose module) allows you to choose which columns get which transformations. Categorical columns will almost always need separate transformations than continuous columns.\n",
    "\n",
    "The `ColumnTransformer` is currently experimental, meaning that its functionality can change in the future. The `ColumnTransformer` takes a list of three-item tuples. The first value in the tuple is a name that labels it, the second is an instantiated estimator, and the third is a list of columns you want to apply the transformation to. The tuple will look like this:\n",
    "\n",
    "```('name', SomeTransformer(parameters), columns)```\n",
    "\n",
    "The columns actually don't have to be column names. Instead, you can use the integer indexes of the columns, a boolean array, or even a function (which accepts the entire DataFrame as the argument and must return a selection of columns).\n",
    "\n",
    "You can also use NumPy arrays with the ColumnTransformer, but this tutorial is focused on the integration of Pandas so we will stick with just using DataFrames.\n",
    "\n",
    "# Pass a Pipeline to the ColumnTransformer\n",
    "We can even pass a pipeline of many transformations to the column transformer, which is what we do here because we have multiple transformations on our string columns.\n",
    "\n",
    "Below, we reproduce the above imputation and encoding using the `ColumnTransformer`. Notice that the pipeline is the exact same as above, just with `cat` appended to each variable name. We will add a different pipeline for the numeric columns in an upcoming section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value='MISSING'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "cat_cols = ['RoofMatl', 'HouseStyle']\n",
    "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers=cat_transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass the entire DataFrame to the ColumnTransformer\n",
    "The ColumnTransformer instance selects the columns we want to use, so we simply pass the entire DataFrame to the fit_transform method. The desired columns will be selected for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_transformed = ct.fit_transform(train)\n",
    "X_cat_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform our test set in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_transformed_test = ct.transform(test)\n",
    "X_cat_transformed_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the feature names\n",
    "We have to do a little digging to get the feature names. All the transformers are stored in the `named_transformers_` dictionary attribute. We then use the names, the first item from the three-item tuple to select the specific transformer. Below, we select our transformer (there is only one here - a pipeline named 'cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = ct.named_transformers_['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from this pipeline we select the one-hot encoder object and finally get the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = pl.named_steps['ohe']\n",
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the numeric columns\n",
    "The numeric columns will need a different set of transformations. Instead of imputing missing values with a constant, the median or mean is often chosen. And instead of encoding the values, we usually standardize them by subtracting the mean of each column and dividing by the standard deviation. This helps many models like ridge regression produce a better fit.\n",
    "\n",
    "## Usually all the numeric columns\n",
    "\n",
    "Instead of selecting just one or two columns by hand like we did above with the string columns, we can select all of the numeric columns. We do this by first finding the data type of each column with the dtypes attribute and then testing whether the kind of each dtype is 'O'. The dtypes attribute returns a Series of NumPy dtype objects. Each of these has a kind attribute that is a single character. We can use this to find the numeric or string columns. Pandas stores all of its string columns as object which have a kind equal to 'O'. See the [NumPy docs][1] for more on the `kind` attribute.\n",
    "\n",
    "[1]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.kind.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the kinds, a one character string representing the dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = np.array([dt.kind for dt in train.dtypes])\n",
    "kinds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume all numeric columns are non-object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train.columns.values\n",
    "is_num = kinds != 'O'\n",
    "num_cols = all_columns[is_num]\n",
    "num_cols[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = all_columns[~is_num]\n",
    "cat_cols[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our numeric column names, we can use the `ColumnTransformer` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
    "num_ss_step = ('ss', StandardScaler())\n",
    "num_steps = [num_si_step, num_ss_step]\n",
    "\n",
    "num_pipe = Pipeline(num_steps)\n",
    "num_transformers = [('num', num_pipe, num_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers=num_transformers)\n",
    "\n",
    "X_num_transformed = ct.fit_transform(train)\n",
    "X_num_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining both categorical and numerical column transformations\n",
    "We can apply separate transformations to each section of our DataFrame with `ColumnTransformer`. We will use every single column in this example. \n",
    "\n",
    "We then create a separate pipeline for both categorical and numerical columns and then use the `ColumnTransformer` to independently transform them. These two transformations happen **in parallel**. The results of each are then concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [('cat', cat_pipe, cat_cols),\n",
    "                ('num', num_pipe, num_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "X = ct.fit_transform(train)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "The whole point of this exercise is to set up our data so that we can do machine learning. We can create one final pipeline and add a machine learning model as the final estimator. The first step in the pipeline will be the entire transformation we just did above. We assigned `y` way back at the top of the tutorial as the `SalePrice`. Here, we will just use the `fit` method instead of `fit_transform` since our final step is a machine learning model and does no transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ml_pipe = Pipeline([('transform', ct), ('ridge', Ridge())])\n",
    "ml_pipe.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipe.score(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "Of course, scoring ourselves on the training set is not useful. Let's do some K-fold cross validation to get an idea of how well we would do with unseen data. We set a random state so that the splits will be the same throughout the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "cross_val_score(ml_pipe, train, y, cv=kf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting parameters when Grid Searching\n",
    "Grid searching in Scikit-Learn requires us to pass a dictionary of parameter names mapped to possible values. When using a pipeline, we must use the name of the step followed by a double-underscore and then the parameter name. If there are multiple layers to your pipeline, as we have here, we must continue using double-underscores to move up a level until we reach the estimator whose parameters we would like to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'transform__num__si__strategy': ['mean', 'median'],\n",
    "    'ridge__alpha': [.001, 0.1, 1.0, 5, 10, 50, 100, 1000],\n",
    "}\n",
    "gs = GridSearchCV(ml_pipe, param_grid, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(train, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all the grid search results in a Pandas DataFrame\n",
    "All the results of the grid search are stored in the `cv_results_` attribute. This is a dictionary that can get converted to a Pandas DataFrame for a nice display and is provides a structure that is much easier to manually scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom transformer that does it all\n",
    "There are a few limitations to the above workflow. For instance, it would be nice if the `OneHotEncoder` gave you the option of ignoring missing values during the fit method. It could simply encode missing values as a row of all zeros. Currently, it forces us to fill the missing values with some string and then encodes this string as a separate column.\n",
    "\n",
    "### Low frequency strings\n",
    "Also, string columns that appear only a few times during the training set may not be reliable predictors in the test set. We may want to encode those as if they were missing as well.\n",
    "\n",
    "### Writing your own estimator class\n",
    "Scikit-Learn provides some help within its documentation on writing your own estimator class. The `BaseEstimator` class found within the base module provides the `get_params` and `set_params` methods for you. The `set_params` method is necessary when doing a grid search. You can write your own or inherit from the `BaseEstimator`. There is also a `TransformerMixin` but it just writes the `fit_transform` method for you. We do this in one line of code below, so we don't inherit from it.\n",
    "\n",
    "The following class BasicTransformer does the following:\n",
    "\n",
    "* Fills in missing values with either the mean or median for numeric columns\n",
    "* Standardizes all numeric columns\n",
    "* Uses one hot encoding for string columns\n",
    "* Does not fill in missing values for categorical columns. Instead it encodes them as a 0's\n",
    "* Ignores unique values in string columns in the test set\n",
    "* Allows you to choose a threshold for the number of occurrences a value must have in a string column. Strings below this threshold will be encoded as all 0's\n",
    "* It only works with DataFrames and is just experimental and not tested so it will break for some datasets\n",
    "* It is called 'basic' because, these are probably the most basic transformations that typically get done to many datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class BasicTransformer(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, cat_threshold=None, num_strategy='median', return_df=False):\n",
    "        # store parameters as public attributes\n",
    "        self.cat_threshold = cat_threshold\n",
    "        \n",
    "        if num_strategy not in ['mean', 'median']:\n",
    "            raise ValueError('num_strategy must be either \"mean\" or \"median\"')\n",
    "        self.num_strategy = num_strategy\n",
    "        self.return_df = return_df\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Assumes X is a DataFrame\n",
    "        self._columns = X.columns.values\n",
    "        \n",
    "        # Split data into categorical and numeric\n",
    "        self._dtypes = X.dtypes.values\n",
    "        self._kinds = np.array([dt.kind for dt in X.dtypes])\n",
    "        self._column_dtypes = {}\n",
    "        is_cat = self._kinds == 'O'\n",
    "        self._column_dtypes['cat'] = self._columns[is_cat]\n",
    "        self._column_dtypes['num'] = self._columns[~is_cat]\n",
    "        self._feature_names = self._column_dtypes['num']\n",
    "        \n",
    "        # Create a dictionary mapping categorical column to unique values above threshold\n",
    "        self._cat_cols = {}\n",
    "        for col in self._column_dtypes['cat']:\n",
    "            vc = X[col].value_counts()\n",
    "            if self.cat_threshold is not None:\n",
    "                vc = vc[vc > self.cat_threshold]\n",
    "            vals = vc.index.values\n",
    "            self._cat_cols[col] = vals\n",
    "            self._feature_names = np.append(self._feature_names, col + '_' + vals)\n",
    "            \n",
    "        # get total number of new categorical columns    \n",
    "        self._total_cat_cols = sum([len(v) for col, v in self._cat_cols.items()])\n",
    "        \n",
    "        # get mean or median\n",
    "        self._num_fill = X[self._column_dtypes['num']].agg(self.num_strategy)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        # check that we have a DataFrame with same column names as the one we fit\n",
    "        if set(self._columns) != set(X.columns):\n",
    "            raise ValueError('Passed DataFrame has different columns than fit DataFrame')\n",
    "        elif len(self._columns) != len(X.columns):\n",
    "            raise ValueError('Passed DataFrame has different number of columns than fit DataFrame')\n",
    "            \n",
    "        # fill missing values    \n",
    "        X_num = X[self._column_dtypes['num']].fillna(self._num_fill)\n",
    "        \n",
    "        # Standardize numerics\n",
    "        std = X_num.std()\n",
    "        X_num = (X_num - X_num.mean()) / std\n",
    "        zero_std = np.where(std == 0)[0]\n",
    "        \n",
    "        # If there is 0 standard deviation, then all values are the same. Set them to 0.\n",
    "        if len(zero_std) > 0:\n",
    "            X_num.iloc[:, zero_std] = 0\n",
    "        X_num = X_num.values\n",
    "        \n",
    "        # create separate array for new encoded categoricals\n",
    "        X_cat = np.empty((len(X), self._total_cat_cols), dtype='int')\n",
    "        i = 0\n",
    "        for col in self._column_dtypes['cat']:\n",
    "            vals = self._cat_cols[col]\n",
    "            for val in vals:\n",
    "                X_cat[:, i] = X[col] == val\n",
    "                i += 1\n",
    "                \n",
    "        # concatenate transformed numeric and categorical arrays\n",
    "        data = np.column_stack((X_num, X_cat))\n",
    "        \n",
    "        # return either a DataFrame or an array\n",
    "        if self.return_df:\n",
    "            return pd.DataFrame(data=data, columns=self._feature_names)\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our `BasicTransformer`\n",
    "Our `BasicTransformer` estimator should be able to be used just like any other scikit-learn estimator. We can instantiate it and then transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = BasicTransformer(cat_threshold=3, return_df=True)\n",
    "train_transformed = bt.fit_transform(train)\n",
    "train_transformed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our transformer in a pipeline\n",
    "Our transformer can be part of pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_pipe = Pipeline([('bt', bt), ('ridge', Ridge())])\n",
    "basic_pipe.fit(train, y)\n",
    "basic_pipe.score(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cross-validate with it as well and get a similar score as we did with our scikit-learn column transformer pipeline from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(basic_pipe, train, y, cv=kf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it as part of a grid search as well. It turns out that not including low-count strings did not help this particular model, though it stands to reason it could in other models. The best score did improve a bit, perhaps due to using a slightly different encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bt__cat_threshold': [0, 1, 2, 3, 4, 5],\n",
    "    'ridge__alpha': [.1, 1, 10, 30, 100]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(basic_pipe, param_grid, cv=kf)\n",
    "gs.fit(train, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning and encoding numeric columns with the new KBinsDiscretizer\n",
    "There are a few columns that contain years. It makes more sense to bin the values in these columns and treat them as categories. Scikit-Learn introduced the new estimator `KBinsDiscretizer` to do just this. It not only bins the values, but it encodes them as well. Before you could of done this manually with Pandas `cut` or `qcut` functions.\n",
    "Let's see how it works with just the `YearBuilt` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "kbd = KBinsDiscretizer(encode='onehot-dense')\n",
    "year_built_transformed = kbd.fit_transform(train[['YearBuilt']])\n",
    "year_built_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, each bin contains (approximately) an equal number of observations. Let's sum up each column to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_built_transformed.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 'quantile' strategy. You can choose 'uniform' to make the bin edges equally spaced or 'kmeans' which uses kmeans clustering to find the bin edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbd.bin_edges_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing all the year columns separately with ColumnTransformer\n",
    "We now have another subset of columns that need separate processing and we can do this with the `ColumnTransformer`. The following code adds one more step to our previous transformation. We also drop the Id column which was just identifying each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\n",
    "not_year = ~np.isin(num_cols, year_cols + ['Id'])\n",
    "num_cols2 = num_cols[not_year]\n",
    "\n",
    "year_si_step = ('si', SimpleImputer(strategy='median'))\n",
    "year_kbd_step = ('kbd', KBinsDiscretizer(n_bins=5, encode='onehot-dense'))\n",
    "year_steps = [year_si_step, year_kbd_step]\n",
    "year_pipe = Pipeline(year_steps)\n",
    "\n",
    "transformers = [('cat', cat_pipe, cat_cols),\n",
    "                ('num', num_pipe, num_cols2),\n",
    "                ('year', year_pipe, year_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers=transformers)\n",
    "X = ct.fit_transform(train)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cross validate and score and see that all this work yielded us no improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipe = Pipeline([('transform', ct), ('ridge', Ridge())])\n",
    "cross_val_score(ml_pipe, train, y, cv=kf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a different number of bins for each column might improve our results. Still, the KBinsDiscretizer makes it easy to bin numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More goodies in Scikit-Learn 0.20\n",
    "There are more new features that come with the upcoming release. Check the What's New section of the docs for more. There are a ton of changes.\n",
    "\n",
    "# Conclusion\n",
    "This article introduced a new workflow that will be available to Scikit-Learn users who rely on Pandas for the initial data exploration and preparation. A much smoother and feature-rich process for taking a Pandas DataFrame and transforming it so that it is ready for machine learning is now done through the new and improved estimators `ColumnTransformer`, `SimpleImputer`, `OneHotEncoder`, and `KBinsDiscretizer`.\n",
    "\n",
    "I am very excited to see this new upgrade and am going to be integrating these new workflows immediately into my projects and teaching materials.\n",
    "\n",
    "If you are interested in taking a more personalized class from me, see my [upcoming public courses][1].\n",
    "\n",
    "[1]: https://www.eventbrite.com/o/dunder-data-9780280058"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
